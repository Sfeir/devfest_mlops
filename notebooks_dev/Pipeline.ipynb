{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4021cd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.0\n",
      "TFX version: 1.9.1\n",
      "KFP version: 1.8.13\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "import kfp\n",
    "print('KFP version: {}'.format(kfp.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9416bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tfx/src/secrets/sfeir-data-394b11f86ed4.json\r\n"
     ]
    }
   ],
   "source": [
    "!find / -name $GOOGLE_APPLICATION_CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d448576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated service account credentials for: [devfest-2022@sfeir-data.iam.gserviceaccount.com]\r\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth activate-service-account --key-file='../secrets/sfeir-data-394b11f86ed4.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef17a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd857d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'sfeir-data'       \n",
    "GOOGLE_CLOUD_REGION = 'europe-west1'         \n",
    "GCS_BUCKET_NAME = 'devfest_mlops_2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c863b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: /content/wine-quality-csv/pipeline_root/\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME = 'wine-quality-csv'\n",
    "\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = '/content/{}/pipeline_root/'.format(PIPELINE_NAME)\n",
    "\n",
    "# Paths for users' Python module.\n",
    "MODULE_ROOT = '/content/{}/pipeline_module/'.format(PIPELINE_NAME)\n",
    "\n",
    "ENDPOINT_NAME = 'prediction-' + PIPELINE_NAME\n",
    "\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1d78cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove '/content/interactive_pipeline': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm -r /content/interactive_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d0de70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = f\"SELECT * FROM `{GOOGLE_CLOUD_PROJECT}.devfest_mlops.winequality_white_binary`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae9e5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "_transformer_module_file = 'wine_quality_transformer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b6e2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wine_quality_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_transformer_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "_NUMERIC_FEATURE_KEYS = [\n",
    "    'fixed_acidity',\t'volatile_acidity',\t'citric_acid',\t'residual_sugar',\n",
    "    'chlorides',\t'free_sulfur_dioxide',\t'total_sulfur_dioxide',\t'density',\n",
    "    'ph',\t'sulphates',\t'alcohol'\n",
    "]\n",
    "_LABEL_KEY = 'quality'\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "    Args:\n",
    "        inputs: map from feature keys to raw not-yet-transformed features.\n",
    "    Returns:\n",
    "        Map from string feature key to transformed feature operations.\n",
    "    \"\"\"\n",
    "\n",
    "    outputs = {}\n",
    "\n",
    "    # scale features to [0,1]\n",
    "    for key in _NUMERIC_FEATURE_KEYS:\n",
    "        scaled = tft.scale_to_0_1(inputs[key])\n",
    "        outputs[key] = tf.reshape(scaled, [-1])\n",
    "\n",
    "    # transform the output\n",
    "    outputs[_LABEL_KEY] = inputs[_LABEL_KEY]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cb3d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://wine_quality_transformer.py...\r\n",
      "/ [0 files][    0.0 B/  843.0 B]                                                \r",
      "/ [1 files][  843.0 B/  843.0 B]                                                \r",
      "\r\n",
      "Operation completed over 1 objects/843.0 B.                                      \r\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {_transformer_module_file} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "999e8405",
   "metadata": {},
   "outputs": [],
   "source": [
    "_trainer_module_file = 'wine_quality_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4b47128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wine_quality_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from keras_tuner.engine import base_tuner\n",
    "\n",
    "_FEATURE_KEYS = [\n",
    "    'fixed_acidity',\t'volatile_acidity',\t'citric_acid',\t'residual_sugar',\n",
    "    'chlorides',\t'free_sulfur_dioxide',\t'total_sulfur_dioxide',\t'density',\n",
    "    'ph',\t'sulphates',\t'alcohol'\n",
    "]\n",
    "_LABEL_KEY = 'quality'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 100\n",
    "_EVAL_BATCH_SIZE = 20\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              tf_transform_output,\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              batch_size: int) -> tf.data.Dataset:\n",
    "\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=tf_transform_output.transformed_metadata.schema).repeat()\n",
    "\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "  \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
    "\n",
    "  # Get transformation graph\n",
    "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "  @tf.function\n",
    "  def serve_tf_examples_fn(serialized_tf_examples):\n",
    "    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "    feature_spec = tf_transform_output.raw_feature_spec()\n",
    "    feature_spec.pop(_LABEL_KEY)\n",
    "\n",
    "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "    transformed_features = model.tft_layer(parsed_features)\n",
    "\n",
    "    return model(transformed_features)\n",
    "\n",
    "  return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def _make_keras_model() -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "  # The model below is built with Functional API, please refer to\n",
    "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
    "\n",
    "  model_layers = keras.layers.concatenate(inputs)\n",
    "  model_layers = keras.layers.Dropout(0.2)(model_layers)\n",
    "  model_layers = keras.layers.BatchNormalization()(model_layers)\n",
    "  model_layers = keras.layers.Dense(units=8, activation='relu')(model_layers)\n",
    "  model_layers = keras.layers.Dropout(0.2)(model_layers)\n",
    "  model_layers = keras.layers.BatchNormalization()(model_layers)\n",
    "  model_layers = keras.layers.Dense(units=14, activation='relu')(model_layers)\n",
    "  model_layers = keras.layers.Dropout(0.2)(model_layers)\n",
    "  model_layers = keras.layers.BatchNormalization()(model_layers)\n",
    "  model_layers = keras.layers.Dense(units=6, activation='relu')(model_layers)\n",
    "  model_layers = keras.layers.Dropout(0.2)(model_layers)\n",
    "  model_layers = keras.layers.BatchNormalization()(model_layers)\n",
    "\n",
    "  outputs = keras.layers.Dense(1)(model_layers)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "  rms_prop_optimizer = tf.keras.optimizers.RMSprop(learning_rate=10e-5)\n",
    "  model.compile(\n",
    "      optimizer=rms_prop_optimizer,\n",
    "      loss=tf.keras.losses.binary_crossentropy,\n",
    "      metrics=['accuracy']\n",
    "  )\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "TunerFnResult = NamedTuple('TunerFnResult', [('tuner', base_tuner.BaseTuner),\n",
    "                                             ('fit_kwargs', Dict[Text, Any])])\n",
    "\n",
    "def tuner_fn(fn_args: FnArgs) -> TunerFnResult:\n",
    "  \"\"\"Build the tuner using the KerasTuner API.\n",
    "  Args:\n",
    "    fn_args: Holds args as name/value pairs.\n",
    "      - working_dir: working dir for tuning.\n",
    "      - train_files: List of file paths containing training tf.Example data.\n",
    "      - eval_files: List of file paths containing eval tf.Example data.\n",
    "      - train_steps: number of train steps.\n",
    "      - eval_steps: number of eval steps.\n",
    "      - schema_path: optional schema of the input data.\n",
    "      - transform_graph_path: optional transform graph produced by TFT.\n",
    "  Returns:\n",
    "    A namedtuple contains the following:\n",
    "      - tuner: A BaseTuner that will be used for tuning.\n",
    "      - fit_kwargs: Args to pass to tuner's run_trial function for fitting the\n",
    "                    model , e.g., the training and validation dataset. Required\n",
    "                    args depend on the above tuner's implementation.\n",
    "  \"\"\"\n",
    "  tuner = Tuner(\n",
    "    module_file=module_file,  # Contains `tuner_fn`.\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=20),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=5))\n",
    "\n",
    "    trainer = Trainer(\n",
    "    module_file=module_file,  # Contains `run_fn`.\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    # This will be passed to `run_fn`.\n",
    "    hyperparameters=tuner.outputs['best_hyperparameters'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=100),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=5))\n",
    "\n",
    "\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      tf_transform_output,\n",
    "      fn_args.data_accessor,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      tf_transform_output,\n",
    "      fn_args.data_accessor,\n",
    "      batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "  model = _make_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "  \n",
    "  # Define default serving signature\n",
    "  signatures = {\n",
    "      'serving_default':\n",
    "          _get_serve_tf_examples_fn(model,\n",
    "                                    tf_transform_output).get_concrete_function(\n",
    "                                        tf.TensorSpec(\n",
    "                                            shape=[None],\n",
    "                                            dtype=tf.string,\n",
    "                                            name='examples'))\n",
    "  }\n",
    "\n",
    "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "  # directory.\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f39b79a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://wine_quality_trainer.py...\r\n",
      "/ [0 files][    0.0 B/  6.2 KiB]                                                \r",
      "/ [1 files][  6.2 KiB/  6.2 KiB]                                                \r",
      "\r\n",
      "Operation completed over 1 objects/6.2 KiB.                                      \r\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {_trainer_module_file} {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "628431e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine_quality_trainer.py  wine_quality_transformer.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44fcc08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.dsl.component.experimental.decorators import component\n",
    "from tfx.dsl.component.experimental.annotations import InputArtifact\n",
    "\n",
    "@component\n",
    "def CreateMonitoringJob(statistics: tfx.dsl.components.InputArtifact[tfx.types.standard_artifacts.ExampleStatistics],\n",
    "                        pushed_model: tfx.dsl.components.InputArtifact[tfx.types.standard_artifacts.PushedModel],\n",
    "                        #pusher_config: tfx.dsl.components.Parameter[str],\n",
    "                        DEFAULT_THRESHOLD_VALUE: tfx.dsl.components.Parameter[float] = 0.03,\n",
    "                        MONITORING_FREQUENCY: tfx.dsl.components.Parameter[int] = 3600,\n",
    "                        SAMPLE_RATE: tfx.dsl.components.Parameter[int] = 0,\n",
    "                        EMAILS: tfx.dsl.components.Parameter[str]= \"\"\n",
    "                        ):#-> tfx.dsl.components.OutputArtifact[tfx.types.standard_artifacts.SchemaGen]\n",
    "    \"\"\"\n",
    "    Creates monitoring job for a model deployed on Vertex endpoint\n",
    "    :param project_id: project id\n",
    "    :param region: resource region\n",
    "    :param endpoint_id: vertex endpoint id\n",
    "    :param deployed_model_id: deployed model id (do not confuse with model id)\n",
    "    :param model_name: model name\n",
    "    :param features_file_uri: filepath to json file in Google Cloud Storage bucket\n",
    "    :return: request response for deploying the monitoring job\n",
    "    \"\"\"\n",
    "    \n",
    "    #from google.cloud import aiplatform\n",
    "    #import json\n",
    "    \n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from google.cloud.aiplatform_v1.services.job_service import JobServiceClient\n",
    "    from google.cloud.aiplatform_v1.types import (SamplingStrategy, ModelMonitoringObjectiveConfig,\n",
    "                                                  ModelMonitoringAlertConfig, ThresholdConfig,\n",
    "                                                  ModelDeploymentMonitoringScheduleConfig,\n",
    "                                                  ModelDeploymentMonitoringObjectiveConfig,\n",
    "                                                  ModelDeploymentMonitoringJob\n",
    "                                                  )\n",
    "    from google.protobuf.duration_pb2 import Duration\n",
    "    \n",
    "    #try:\n",
    "    pushed_destination = pushed_model.get()[0].__dict__['_artifact'].custom_properties['pushed_destination'].string_value.split('/')\n",
    "    project_id = pushed_destination[1]\n",
    "    region = pushed_destination[3]\n",
    "    #except:\n",
    "    #    project_id=json.loads(pusher_config)['ai_platform_serving_args']['project_id']\n",
    "    #    region=json.loads(pusher_config)['ai_platform_vertex_region']\n",
    "    \n",
    "    #aiplatform.init(project=project_id, location=region)\n",
    "    \n",
    "    #endpoint = aiplatform.Endpoint.list(\n",
    "      #filter=f\"display_name={deployed_model_display_name}\",\n",
    "    #  order_by=\"update_time\"\n",
    "    #  )[-1]\n",
    "    \n",
    "    model = aiplatform.Model(pushed_model.get()[0].__dict__['_artifact'].custom_properties['pushed_destination'].string_value)\n",
    "    #pusher_config = json.loads(pusher.exec_properties['custom_config'])\n",
    "\n",
    "    model_name=model.display_name\n",
    "    #endpoint.gca_resource.deployed_models[0].display_name\n",
    "    endpoint_id = model.to_dict()['deployedModels'][0]['endpoint']\n",
    "    #endpoint_id=endpoint.gca_resource.name.split('/')[-1]\n",
    "    deployed_model_id = model.to_dict()['deployedModels'][0]['deployedModelId']\n",
    "    #deployed_model_id=endpoint.list_models()[0].id\n",
    "    \n",
    "    # try:\n",
    "    features_file_uri=statistics._artifacts[0].uri\n",
    "    # except:\n",
    "    #    features_file_uri='/content/interactive_pipeline/StatisticsGen/statistics/2'\n",
    "    \n",
    "    \n",
    "    def get_features_names(features_file_uri: str):\n",
    "        \"\"\"\n",
    "        Reads and parses json file containing features names from GCS bucket\n",
    "        :param features_file_uri:\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        def load_statistics(file_path: str):\n",
    "          return tfdv.load_stats_binary(file_path)\n",
    "\n",
    "        return load_statistics(features_file_uri+'/Split-train/FeatureStats.pb')\n",
    "    \n",
    "    stats=get_features_names(features_file_uri)\n",
    "    api_vertex_endpoint = f\"{region.upper()}-aiplatform.googleapis.com\"\n",
    "\n",
    "    features = {}\n",
    "    for feature in stats.datasets[0].features:\n",
    "      features[feature.path.step[0]]:{'mean':feature.num_stats.mean, 'std_dev':feature.num_stats.std_dev}\n",
    "    #numerical_features, categorical_features = get_features_names(features_file_uri)\n",
    "\n",
    "    sampling_config = SamplingStrategy.RandomSampleConfig(sample_rate=SAMPLE_RATE)\n",
    "    sampling_strategy = SamplingStrategy(random_sample_config=sampling_config)\n",
    "\n",
    "    monitoring_duration = Duration(seconds=MONITORING_FREQUENCY)\n",
    "    monitoring_config = ModelDeploymentMonitoringScheduleConfig(monitor_interval=monitoring_duration)\n",
    "\n",
    "    email_config = ModelMonitoringAlertConfig.EmailAlertConfig(user_emails=[EMAILS])\n",
    "    alert_config = ModelMonitoringAlertConfig(email_alert_config=email_config)\n",
    "\n",
    "    # monitoring whether feature data distribution changes significantly over time\n",
    "    drift_thresholds = {}\n",
    "    default_threshold = ThresholdConfig(value=DEFAULT_THRESHOLD_VALUE)\n",
    "\n",
    "    # set thresholds as default for all features\n",
    "    for feature in features:\n",
    "        drift_thresholds[feature] = 1,96*features[feature]['std_dev']+features[feature]['mean']\n",
    "\n",
    "    drift_config = ModelMonitoringObjectiveConfig.PredictionDriftDetectionConfig(\n",
    "        drift_thresholds=drift_thresholds\n",
    "    )\n",
    "\n",
    "    objective_config = ModelMonitoringObjectiveConfig(\n",
    "        prediction_drift_detection_config=drift_config\n",
    "    )\n",
    "    monitoring_objective_configs = ModelDeploymentMonitoringObjectiveConfig(\n",
    "        objective_config=objective_config\n",
    "    )\n",
    "    monitoring_objective_configs.deployed_model_id = deployed_model_id\n",
    "\n",
    "    # create the monitoring job\n",
    "    #endpoint = f\"projects/{project_id}/locations/{region}/endpoints/{endpoint_id}\"\n",
    "    predict_schema = \"\"\n",
    "    analysis_schema = \"\"\n",
    "\n",
    "    # monitoring job will  create up to 4 bq tables with names :\n",
    "    #   bq://<project_id>.model_deployment_monitoring_<endpoint_id>.<tolower(log_source)>_<tolower(log_type)>\n",
    "    # https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform_v1beta1.types.ModelDeploymentMonitoringJob\n",
    "    monitoring_job = ModelDeploymentMonitoringJob(\n",
    "        display_name=f\"monitoring_{model_name}\",\n",
    "        endpoint=endpoint_id,\n",
    "        model_deployment_monitoring_objective_configs=[monitoring_objective_configs],\n",
    "        logging_sampling_strategy=sampling_strategy,\n",
    "        model_deployment_monitoring_schedule_config=monitoring_config,\n",
    "        model_monitoring_alert_config=alert_config,\n",
    "        predict_instance_schema_uri=predict_schema,\n",
    "        analysis_instance_schema_uri=analysis_schema,\n",
    "        enable_monitoring_pipeline_logs=True\n",
    "    )\n",
    "    options = dict(api_endpoint=api_vertex_endpoint)\n",
    "    client = JobServiceClient(client_options=options)\n",
    "    parent = f\"projects/{project_id}/locations/{region}\"\n",
    "    response = client.create_model_deployment_monitoring_job(\n",
    "        parent=parent, model_deployment_monitoring_job=monitoring_job\n",
    "    )\n",
    "\n",
    "#    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76f33ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, query: str,\n",
    "                     transformer_module_file:str,\n",
    "                     trainer_module_file: str,\n",
    "                     endpoint_name: str,\n",
    "                     project_id: str, region: str,\n",
    "                     beam_pipeline_args: Optional[List[str]],\n",
    "                     ) -> tfx.dsl.Pipeline:\n",
    "    \"\"\"Creates a TFX pipeline using BigQuery.\"\"\"\n",
    "\n",
    "    # query data in BigQuery as a data source.\n",
    "    output = tfx.proto.Output(\n",
    "             split_config=tfx.proto.SplitConfig(splits=[\n",
    "                 tfx.proto.SplitConfig.Split(name='train', hash_buckets=4),\n",
    "                 tfx.proto.SplitConfig.Split(name='eval', hash_buckets=1)\n",
    "             ]))\n",
    "\n",
    "    example_gen = tfx.extensions.google_cloud_big_query.BigQueryExampleGen(\n",
    "      query=query, output_config=output)\n",
    "\n",
    "    # compute the statistics\n",
    "    statistics_gen = tfx.components.StatisticsGen(\n",
    "      examples=example_gen.outputs['examples'])\n",
    "\n",
    "    # generate schema\n",
    "    schema_gen = tfx.components.SchemaGen(\n",
    "      statistics=statistics_gen.outputs['statistics'])\n",
    "\n",
    "    # validation component for inference examples\n",
    "    validator = tfx.components.ExampleValidator(\n",
    "      statistics=statistics_gen.outputs['statistics'],\n",
    "      schema=schema_gen.outputs['schema']\n",
    "      )\n",
    "\n",
    "    # pre-processe data\n",
    "    transformer = tfx.components.Transform(\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      schema=schema_gen.outputs['schema'],\n",
    "      module_file=transformer_module_file)\n",
    "\n",
    "    # use user-provided Python function that trains a model\n",
    "    trainer = tfx.components.Trainer(\n",
    "      module_file=trainer_module_file,\n",
    "      examples=transformer.outputs['transformed_examples'],\n",
    "      transform_graph=transformer.outputs['transform_graph'],\n",
    "      schema=schema_gen.outputs['schema'],\n",
    "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
    "      eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
    "\n",
    "    # push the model to model registry\n",
    "    vertex_serving_spec = {\n",
    "      'project_id': project_id,\n",
    "      'endpoint_name': endpoint_name,\n",
    "      'machine_type': 'n1-standard-4'\n",
    "    }\n",
    "    serving_image = 'europe-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest'\n",
    "\n",
    "    pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      custom_config={\n",
    "          tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n",
    "              True,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n",
    "              region,\n",
    "          tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY:\n",
    "              serving_image,\n",
    "          tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY:\n",
    "            vertex_serving_spec,\n",
    "      })\n",
    "\n",
    "    # Create the monitoring job\n",
    "    monitoring = CreateMonitoringJob(statistics=statistics_gen.outputs['statistics'],\n",
    "                                  pushed_model=pusher.outputs['pushed_model'])\n",
    "  \n",
    "\n",
    "    components = [\n",
    "      example_gen,\n",
    "      statistics_gen,\n",
    "      schema_gen,\n",
    "      validator,\n",
    "      transformer,\n",
    "      trainer,\n",
    "      pusher,\n",
    "      monitoring\n",
    "    ]\n",
    "\n",
    "    return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      components=components,\n",
    "      beam_pipeline_args=beam_pipeline_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ccc7c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=../secrets/sfeir-data-394b11f86ed4.json\n",
      "../secrets/sfeir-data-394b11f86ed4.json\n"
     ]
    }
   ],
   "source": [
    "%env GOOGLE_APPLICATION_CREDENTIALS ../secrets/sfeir-data-394b11f86ed4.json\n",
    "!echo $GOOGLE_APPLICATION_CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de270e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://devfest_mlops_2022/pipeline_module/\r\n",
      "gs://devfest_mlops_2022/pipeline_root/\r\n",
      "gs://devfest_mlops_2022/winequality/\r\n",
      "gs://devfest_mlops_2022/winequality_white_wine/\r\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://devfest_mlops_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f0c137d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content/wine-quality-csv/pipeline_root/'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42677d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Generating ephemeral wheel package for '/content/wine-quality-csv/pipeline_module/wine_quality_transformer.py' (including modules: ['wine_quality_transformer', 'wine_quality_trainer']).\n",
      "INFO:absl:User module package has hash fingerprint version c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.\n",
      "INFO:absl:Executing: ['/usr/bin/python3', '/tmp/tmpen4bgv_z/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpdfe10qvf', '--dist-dir', '/tmp/tmp36ea0nwj']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying wine_quality_transformer.py -> build/lib\n",
      "copying wine_quality_trainer.py -> build/lib\n",
      "installing to /tmp/tmpdfe10qvf\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/wine_quality_transformer.py -> /tmp/tmpdfe10qvf\n",
      "copying build/lib/wine_quality_trainer.py -> /tmp/tmpdfe10qvf\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Transform.egg-info\n",
      "writing tfx_user_code_Transform.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Transform.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Transform.egg-info to /tmp/tmpdfe10qvf/tfx_user_code_Transform-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpdfe10qvf/tfx_user_code_Transform-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.dist-info/WHEEL\n",
      "creating '/tmp/tmp36ea0nwj/tfx_user_code_Transform-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8-py3-none-any.whl' and adding '/tmp/tmpdfe10qvf' to it\n",
      "adding 'wine_quality_trainer.py'\n",
      "adding 'wine_quality_transformer.py'\n",
      "adding 'tfx_user_code_Transform-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Transform-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Transform-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Transform-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.dist-info/RECORD'\n",
      "removing /tmp/tmpdfe10qvf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "INFO:absl:Successfully built user code wheel distribution at 'gs://devfest_mlops_2022/pipeline_root/_wheels/tfx_user_code_Transform-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8-py3-none-any.whl'; target user module is 'wine_quality_transformer'.\n",
      "INFO:absl:Full user module path is 'wine_quality_transformer@gs://devfest_mlops_2022/pipeline_root/_wheels/tfx_user_code_Transform-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8-py3-none-any.whl'\n",
      "INFO:absl:Generating ephemeral wheel package for '/content/wine-quality-csv/pipeline_module/wine_quality_trainer.py' (including modules: ['wine_quality_transformer', 'wine_quality_trainer']).\n",
      "INFO:absl:User module package has hash fingerprint version c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.\n",
      "INFO:absl:Executing: ['/usr/bin/python3', '/tmp/tmpx55xflfu/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpfkwxf5kb', '--dist-dir', '/tmp/tmplvjbtxa9']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying wine_quality_transformer.py -> build/lib\n",
      "copying wine_quality_trainer.py -> build/lib\n",
      "installing to /tmp/tmpfkwxf5kb\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/wine_quality_transformer.py -> /tmp/tmpfkwxf5kb\n",
      "copying build/lib/wine_quality_trainer.py -> /tmp/tmpfkwxf5kb\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /tmp/tmpfkwxf5kb/tfx_user_code_Trainer-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8-py3.7.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpfkwxf5kb/tfx_user_code_Trainer-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.dist-info/WHEEL\n",
      "creating '/tmp/tmplvjbtxa9/tfx_user_code_Trainer-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8-py3-none-any.whl' and adding '/tmp/tmpfkwxf5kb' to it\n",
      "adding 'wine_quality_trainer.py'\n",
      "adding 'wine_quality_transformer.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8.dist-info/RECORD'\n",
      "removing /tmp/tmpfkwxf5kb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  setuptools.SetuptoolsDeprecationWarning,\n",
      "INFO:absl:Successfully built user code wheel distribution at 'gs://devfest_mlops_2022/pipeline_root/_wheels/tfx_user_code_Trainer-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8-py3-none-any.whl'; target user module is 'wine_quality_trainer'.\n",
      "INFO:absl:Full user module path is 'wine_quality_trainer@gs://devfest_mlops_2022/pipeline_root/_wheels/tfx_user_code_Trainer-0.0+c1150eedbb85fec0209086ea8cfe31f83b43deb0437299552bc480ac495bccf8-py3-none-any.whl'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS = [\n",
    "   '--project=' + GOOGLE_CLOUD_PROJECT,\n",
    "   '--temp_location=' + os.path.join('gs://', GCS_BUCKET_NAME, 'tmp'),\n",
    "   ]\n",
    "\n",
    "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
    "\n",
    "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
    "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE\n",
    ")\n",
    "_ = runner.run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root='gs://devfest_mlops_2022/pipeline_root',\n",
    "        query=QUERY,\n",
    "        transformer_module_file=os.path.join(MODULE_ROOT, _transformer_module_file),\n",
    "        trainer_module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n",
    "        endpoint_name=ENDPOINT_NAME,\n",
    "        project_id=GOOGLE_CLOUD_PROJECT,\n",
    "        region=GOOGLE_CLOUD_REGION,\n",
    "        beam_pipeline_args=BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3526ca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n"
     ]
    },
    {
     "ename": "InvalidArgument",
     "evalue": "400 You do not have permission to act as service_account: 881499317005-compute@developer.gserviceaccount.com. (or it may not exist).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"You do not have permission to act as service_account: 881499317005-compute@developer.gserviceaccount.com. (or it may not exist).\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:216.58.213.170:443 {created_time:\"2022-09-20T11:03:50.3116121+00:00\", grpc_status:3, grpc_message:\"You do not have permission to act as service_account: 881499317005-compute@developer.gserviceaccount.com. (or it may not exist).\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2321/504383612.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n\u001b[1;32m      9\u001b[0m                                 display_name=PIPELINE_NAME)\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/aiplatform/pipeline_jobs.py\u001b[0m in \u001b[0;36msubmit\u001b[0;34m(self, service_account, network, create_request_timeout, experiment)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mpipeline_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mpipeline_job_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         )\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/aiplatform_v1/services/pipeline_service/client.py\u001b[0m in \u001b[0;36mcreate_pipeline_job\u001b[0;34m(self, request, parent, pipeline_job, pipeline_job_id, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m         )\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 You do not have permission to act as service_account: 881499317005-compute@developer.gserviceaccount.com. (or it may not exist)."
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "aiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n",
    "\n",
    "job = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n",
    "                                display_name=PIPELINE_NAME)\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3518976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def serialize_example(alcohol, chlorides, citric_acid, density, fixed_acidity, free_sulfur_dioxide,\n",
    "                      ph, residual_sugar, sulphates, total_sulfur_dioxide, volatile_acidity):\n",
    "  \"\"\"\n",
    "  Creates a tf.train.Example message ready to be written to a file.\n",
    "  \"\"\"\n",
    "  # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "  # data type.\n",
    "  feature = {\n",
    "      'alcohol': _float_feature(alcohol),\n",
    "      'chlorides': _float_feature(chlorides),\n",
    "      'citric_acid': _float_feature(citric_acid),\n",
    "      'density': _float_feature(density),\n",
    "      'fixed_acidity': _float_feature(fixed_acidity),\n",
    "      'free_sulfur_dioxide': _float_feature(free_sulfur_dioxide),\n",
    "      'ph': _float_feature(ph),\n",
    "      'residual_sugar': _float_feature(residual_sugar),\n",
    "      'sulphates': _float_feature(sulphates),\n",
    "      'total_sulfur_dioxide': _float_feature(total_sulfur_dioxide),\n",
    "      'volatile_acidity': _float_feature(volatile_acidity),\n",
    "  }\n",
    "\n",
    "  # Create a Features message using tf.train.Example.\n",
    "\n",
    "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "  return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86c95225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sfeir-data-394b11f86ed4.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55c83c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"type\": \"service_account\",\r\n",
      "  \"project_id\": \"sfeir-data\",\r\n",
      "  \"private_key_id\": \"394b11f86ed402dd6f3216df56a38805086f118c\",\r\n",
      "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCf4L6pnPqw+wlO\\nPeq/55DDP4lTv6TDTu/7p31WcARO3X3/W+MEg5SntcH1AWKHxhnza+rhWJzNUOrb\\nY/7r0AXeojsEUQ+/+LmTvk6KxBrQgolufobajCP8ryJ3p7zvVO6mFeRWf0uBnepa\\nImVRSbKUdCdKlELOYjRVS1IkDjc4WC1Jve/zpAQIkKmyBXOVAEe8Sl7w2DafPK1Z\\nJuv4g2Jx5PNZfE/O+sjV8x3X7c9b037Z9YEuoAhau9X0TJ/bgCRsjViiv7PIl6a/\\n6MXoAWBICz7I36jwj0SO9+Ps8sdO3oOJByj33WEB4dORSW7qPFU6aAWXXOkXzNGw\\nKoHFwgjfAgMBAAECggEAOJlWxjMVqMvmWnWfAnsXC5gVpLlmueHbIYsE2zHixIyz\\nC1DbSIDQgqLc3EC2QfzGuR9OUqKNOD5aNnrrB05olY13OSr9WWfTQtOPeJ6+I2zi\\n/GP8l6gfIVha6mNOhqfZqKB9aoW8FFH0Yh5lIebmOpn4QvXDxM6NWljE5pDFycMW\\nU0E9p97BJ+cCBA/Y10JIQoQSKx1sNizB6uVmyaucSojmp0rtOGOMIfS1Box+hxA6\\nQW9e9gPZGK2Yj5g0Mh+INlH3lN+rwQ5H94YviVac0J0qOWQcfzPhehPQjgh+ptWC\\nlaRSFHHO/VH4JnEMmcLUewQAxyIsxm/ru1xrHYOQEQKBgQDPkH5beTW7e/Jqk7bU\\nHXkc0kwGJNCFVQ3KJuLRz6SeCzc6aIZQXRsK7D3roJXSexEfobrceKpRKRI5WP+h\\n6xI2uEmcAg4KIfz5Sdqo8ptU5kaJFSZ2CS1XGtmlbgg5oGJtiUc+FvbIRM0cxqNq\\nUiUOv/ewZT2C9tgGE7O2fj8lmQKBgQDFL4x6GxIESSHiFwPhLy05RYGFyqfCX/bx\\nrUIyYOV/lJP0uKuI4YTFkKHMb4i/CYhaA+vEmET4vcDyLy5dJeRln5W7S1qsuKT4\\n2P7Rvrk0wcaIPF1ofheEiu7pi++Q/PfC8cJ1WMwqV4kP/Gjgx6G3YUPQF09/R455\\nou2lqke9NwKBgBMCVrAl76fV+COOl6XhdSAQmmnNoVM0sOicmxVAAQGAYXR18icV\\n/84GSL11nYMDRlQxSZ27Z027hzG9VtwLXUWppwQkpAiFiFeETgQ/A77IwBj52OXi\\nfUaG7PvbkeFFMP57Hg0vqTY6JtbF0l2AXGh3aEW9X97IoIwd7c1+zmWxAoGBAIfW\\nQKShX2D6bFAO4MpQsIvCmP5s4/JgH4LPg0mnPJIN7XrpeKmcCdX706o8r2xDCd5Q\\n0yskZmcgePmdjQf4IYXsUL6so3NW47bV7XzaMUXps3WIeCSSsGrLNthGYSt1SMzP\\ntNEX8dW9ZZtQm4M1ou7sH0YLuOUKi1i555J3YmIzAoGAEsp4af+eSQCk46U3IspS\\nI9CFCYLtC3j5hGFES+wzgPOD8+6qeiBR1DiHeYKsXP97ZUtK8OsmB510Ntw4pzp1\\nLbrtXYz64UbXHYVK51/gKw+Ax3QnsWCoHzg/jUTpjTIYtb4VzMpZHymWSi7frYma\\nnyOUPJBTztnEB2hx9Uqz/hk=\\n-----END PRIVATE KEY-----\\n\",\r\n",
      "  \"client_email\": \"devfest-2022@sfeir-data.iam.gserviceaccount.com\",\r\n",
      "  \"client_id\": \"117891307820419548375\",\r\n",
      "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\r\n",
      "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\r\n",
      "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\r\n",
      "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/devfest-2022%40sfeir-data.iam.gserviceaccount.com\"\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../secrets/sfeir-data-394b11f86ed4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74a0ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\n",
    "    'api_endpoint': GOOGLE_CLOUD_REGION + '-aiplatform.googleapis.com'\n",
    "    }\n",
    "\n",
    "# Initialize client that will be used to create and send requests.\n",
    "client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "\n",
    "ENDPOINT_ID='7290923176433811456'\n",
    "\n",
    "endpoint = client.endpoint_path(\n",
    "        project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION, endpoint=ENDPOINT_ID\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c915ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "instances = [{\n",
    "      \"examples\":{'b64': \n",
    "      base64.b64encode(serialize_example(13.8, 0.036, 0.0, 0.98981, 4.7, \n",
    "                                         23.0, 3.53, 3.4, 0.92, 134.0, 0.785)).decode()}\n",
    "      }]\n",
    "response = client.predict(endpoint=endpoint, instances=instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a559c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions {\n",
       "  list_value {\n",
       "    values {\n",
       "      number_value: -0.130293861\n",
       "    }\n",
       "  }\n",
       "}\n",
       "deployed_model_id: \"41464782506688512\"\n",
       "model: \"projects/881499317005/locations/europe-west1/models/1821934747689943040\"\n",
       "model_display_name: \"v1663106292\"\n",
       "model_version_id: \"1\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa0016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
